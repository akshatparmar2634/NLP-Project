{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-14T10:44:06.195703Z",
          "iopub.status.idle": "2025-04-14T10:44:06.195911Z",
          "shell.execute_reply": "2025-04-14T10:44:06.195825Z",
          "shell.execute_reply.started": "2025-04-14T10:44:06.195816Z"
        },
        "id": "TittN4Vl6fiC",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
            "GPU memory: 8.585216 GB\n",
            "Using device: cuda:0\n",
            "Loaded 189 training games\n",
            "Processed 13128 messages with 590 deceptive and 12538 truthful\n",
            "Processed 1416 messages with 56 deceptive and 1360 truthful\n",
            "Processed 2741 messages with 240 deceptive and 2501 truthful\n",
            "Processed 13128 messages with 590 deceptive and 12538 truthful\n",
            "Processed 1416 messages with 56 deceptive and 1360 truthful\n",
            "Processed 2741 messages with 240 deceptive and 2501 truthful\n",
            "Vocabulary size: 4650\n",
            "Positive weight (truthful/deceptive): 21.2508\n",
            "\n",
            "Starting Epoch 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 411/411 [00:08<00:00, 50.49it/s, batch=411/411, loss=3.3417, avg_loss=1.3294]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.3294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]c:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
            "  output = torch._nested_tensor_from_mask(\n",
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 225.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0761\n",
            "Best threshold: 0.5452, Best F1: 0.1190\n",
            "New best model saved with F1: 0.1190\n",
            "\n",
            "Starting Epoch 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 411/411 [00:07<00:00, 53.44it/s, batch=411/411, loss=0.7202, avg_loss=1.2983]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 1.2983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 287.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0761\n",
            "Best threshold: 0.5864, Best F1: 0.1351\n",
            "New best model saved with F1: 0.1351\n",
            "\n",
            "Starting Epoch 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 411/411 [00:07<00:00, 53.99it/s, batch=411/411, loss=0.7539, avg_loss=1.2847]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Loss: 1.2847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 258.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0761\n",
            "Best threshold: 0.5761, Best F1: 0.1370\n",
            "New best model saved with F1: 0.1370\n",
            "\n",
            "Starting Epoch 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training: 100%|██████████| 411/411 [00:07<00:00, 53.34it/s, batch=411/411, loss=2.3014, avg_loss=1.2670]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Loss: 1.2670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 256.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0761\n",
            "Best threshold: 0.5759, Best F1: 0.1287\n",
            "\n",
            "Starting Epoch 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 411/411 [00:07<00:00, 51.70it/s, batch=411/411, loss=1.7109, avg_loss=1.2162]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Loss: 1.2162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 196.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0784\n",
            "Best threshold: 0.5738, Best F1: 0.1505\n",
            "New best model saved with F1: 0.1505\n",
            "\n",
            "Starting Epoch 6...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 411/411 [00:07<00:00, 51.64it/s, batch=411/411, loss=0.8482, avg_loss=1.1583]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Loss: 1.1583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 186.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0793\n",
            "Best threshold: 0.6212, Best F1: 0.1618\n",
            "New best model saved with F1: 0.1618\n",
            "\n",
            "Starting Epoch 7...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 411/411 [00:07<00:00, 51.98it/s, batch=411/411, loss=1.9822, avg_loss=1.1081]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Loss: 1.1081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 179.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0858\n",
            "Best threshold: 0.5868, Best F1: 0.1519\n",
            "\n",
            "Starting Epoch 8...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 411/411 [00:08<00:00, 51.26it/s, batch=411/411, loss=0.4386, avg_loss=1.0523]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Loss: 1.0523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 190.69it/s]\n",
            "C:\\Users\\Akshat\\AppData\\Local\\Temp\\ipykernel_23952\\2185088981.py:406: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1 (threshold=0.5): 0.0879\n",
            "Best threshold: 0.5891, Best F1: 0.1529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 45/45 [00:00<00:00, 243.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Set Performance\n",
            "Accuracy: 0.8531, F1: 0.1261, AUC: 0.6365\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.88      0.92      1360\n",
            "         1.0       0.08      0.27      0.13        56\n",
            "\n",
            "    accuracy                           0.85      1416\n",
            "   macro avg       0.52      0.57      0.52      1416\n",
            "weighted avg       0.93      0.85      0.89      1416\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 86/86 [00:00<00:00, 245.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Set Performance\n",
            "Accuracy: 0.8179, F1: 0.2067, AUC: 0.6287\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.87      0.90      2501\n",
            "         1.0       0.17      0.27      0.21       240\n",
            "\n",
            "    accuracy                           0.82      2741\n",
            "   macro avg       0.55      0.57      0.55      2741\n",
            "weighted avg       0.86      0.82      0.84      2741\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"GPU memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
        "\n",
        "# -------------------------r\n",
        "# 1. Tokenization and Vocab\n",
        "# -------------------------\n",
        "def diplomacy_tokenizer(text):\n",
        "    \"\"\"Tokenize text, preserving punctuation as separate tokens.\"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = re.findall(r\"\\w+|[.,!?;]\", text)\n",
        "    return tokens\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, min_freq=2):\n",
        "        self.token_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx_to_token = ['<PAD>', '<UNK>']\n",
        "        self.min_freq = min_freq\n",
        "        self.harbinger_indices = set()  # Store harbinger token indices\n",
        "\n",
        "    def build_vocab(self, texts, harbingers):\n",
        "        counter = Counter()\n",
        "        for text in texts:\n",
        "            counter.update(diplomacy_tokenizer(text))\n",
        "        for token, freq in counter.items():\n",
        "            if freq >= self.min_freq:\n",
        "                self.idx_to_token.append(token)\n",
        "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "        # Identify harbinger indices\n",
        "        self.harbinger_indices = set([self.token_to_idx[token] for token in harbingers\n",
        "                                    if token in self.token_to_idx])\n",
        "\n",
        "    def encode(self, tokens):\n",
        "        return [self.token_to_idx.get(t, 1) for t in tokens]  # 1 = <UNK>\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "# -------------------\n",
        "# 2. Custom Dataset\n",
        "# -------------------\n",
        "class DiplomacyDataset(Dataset):\n",
        "    def __init__(self, messages, labels, vocab, context_size=2, max_len=300):\n",
        "        self.messages = messages\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.context_size = context_size\n",
        "        self.max_len = max_len\n",
        "        self.encoded = [vocab.encode(diplomacy_tokenizer(msg)) for msg in messages]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.messages)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = []\n",
        "        for i in range(max(0, idx - self.context_size), idx + 1):\n",
        "            context.extend(self.encoded[i])\n",
        "        if len(context) < self.max_len:\n",
        "            context += [0] * (self.max_len - len(context))\n",
        "        else:\n",
        "            context = context[:self.max_len]\n",
        "        # Create harbinger mask\n",
        "        harbinger_mask = [1 if token in self.vocab.harbinger_indices else 0\n",
        "                         for token in context]\n",
        "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
        "        harbinger_mask_tensor = torch.tensor(harbinger_mask, dtype=torch.float)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return context_tensor, harbinger_mask_tensor, label\n",
        "\n",
        "# -------------------\n",
        "# 3. Positional Encoding (Fixed)\n",
        "# -------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # Handle both even and odd dimensions\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 0:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:-1])  # Skip last when odd\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# -------------------\n",
        "# 4. Transformer-based Model with Harbingers (Fixed)\n",
        "# -------------------\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2, hidden_dim=256, dropout=0.3, max_len=300):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        # Ensure embed_dim + 1 is divisible by num_heads\n",
        "        assert (embed_dim + 1) % num_heads == 0, \"embed_dim + 1 must be divisible by num_heads\"\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        d_model = embed_dim + 1  # Add 1 for harbinger feature\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x, harb_mask):\n",
        "        # x: (batch_size, max_len)\n",
        "        embedded = self.embedding(x)  # (batch_size, max_len, embed_dim)\n",
        "        harb_mask = harb_mask.unsqueeze(2)  # (batch_size, max_len, 1)\n",
        "        combined = torch.cat([embedded, harb_mask], dim=2)  # (batch_size, max_len, d_model)\n",
        "        combined = self.pos_encoder(combined)  # (batch_size, max_len, d_model)\n",
        "        # Create mask for padding tokens\n",
        "        mask = (x == 0)  # (batch_size, max_len)\n",
        "        output = self.transformer_encoder(combined, src_key_padding_mask=mask)  # (batch_size, max_len, d_model)\n",
        "        output = output.mean(dim=1)  # (batch_size, d_model)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output).squeeze(1)  # (batch_size,)\n",
        "        return output\n",
        "\n",
        "# -------------------\n",
        "# 5. Training + Eval\n",
        "# -------------------\n",
        "# Add these imports at the top\n",
        "import torch.cuda as cuda\n",
        "import gc\n",
        "\n",
        "# Modify the train_epoch function\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    total_batches = len(dataloader)\n",
        "    \n",
        "    # Clear GPU memory before training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    \n",
        "    pbar = tqdm(enumerate(dataloader), total=total_batches, desc='Training')\n",
        "    \n",
        "    for batch_idx, (x, harb_mask, y) in pbar:\n",
        "        try:\n",
        "            x = x.to(device)\n",
        "            harb_mask = harb_mask.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x, harb_mask)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            current_loss = loss.item()\n",
        "            losses.append(current_loss)\n",
        "            \n",
        "            # Update progress bar with current batch and loss\n",
        "            pbar.set_postfix({\n",
        "                'batch': f'{batch_idx+1}/{total_batches}',\n",
        "                'loss': f'{current_loss:.4f}',\n",
        "                'avg_loss': f'{np.mean(losses):.4f}'\n",
        "            })\n",
        "            \n",
        "            # Clear some memory\n",
        "            del x, harb_mask, y, logits, loss\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    return np.mean(losses)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for x, harb_mask, y in tqdm(dataloader, desc='Evaluating'):\n",
        "            x = x.to(device)\n",
        "            harb_mask = harb_mask.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(x, harb_mask)\n",
        "            all_logits.extend(logits.cpu().tolist())\n",
        "            all_labels.extend(y.cpu().tolist())\n",
        "    return np.array(all_logits), np.array(all_labels)\n",
        "\n",
        "def print_metrics(true, preds, probs, name=\"\"):\n",
        "    acc = accuracy_score(true, preds)\n",
        "    f1 = f1_score(true, preds)\n",
        "    auc = roc_auc_score(true, probs)\n",
        "    print(f\"\\n{name} Set Performance\")\n",
        "    print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
        "    print(classification_report(true, preds))\n",
        "\n",
        "# -------------------\n",
        "# 6. Threshold Adjustment\n",
        "# -------------------\n",
        "def find_best_threshold(logits, labels):\n",
        "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "    precision, recall, thresholds = precision_recall_curve(labels, probs)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_threshold = thresholds[best_idx]\n",
        "    best_f1 = f1_scores[best_idx]\n",
        "    return best_threshold, best_f1\n",
        "\n",
        "# -------------------\n",
        "# 7. Run Training\n",
        "# -------------------\n",
        "def load_jsonl(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "def preprocess_messages(data):\n",
        "    messages, labels = [], []\n",
        "    for game in data:\n",
        "        game_messages = game.get(\"messages\", [])\n",
        "        sender_labels = game.get(\"sender_labels\", [])\n",
        "        if not game_messages or len(game_messages) < 2 or len(sender_labels) != len(game_messages):\n",
        "            continue\n",
        "        for i in range(len(game_messages)):\n",
        "            is_deceptive = 0 if sender_labels[i] else 1  # 1 = deceptive, 0 = truthful\n",
        "            messages.append(game_messages[i])\n",
        "            labels.append(is_deceptive)\n",
        "\n",
        "    num_deceptive = sum(labels)\n",
        "    num_truthful = len(labels) - num_deceptive\n",
        "    print(f\"Processed {len(messages)} messages with {num_deceptive} deceptive and {num_truthful} truthful\")\n",
        "    return messages, labels, num_truthful, num_deceptive\n",
        "\n",
        "def run_training():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data from local files\n",
        "    train_data = load_jsonl(\"data/train.jsonl\")\n",
        "    val_data = load_jsonl(\"data/validation.jsonl\") \n",
        "    test_data = load_jsonl(\"data/test.jsonl\")\n",
        "\n",
        "    print(f\"Loaded {len(train_data)} training games\")\n",
        "    \n",
        "    # Rest of the function remains the same\n",
        "    train_msgs, train_labels, train_num_truthful, train_num_deceptive = preprocess_messages(train_data)\n",
        "    val_msgs, val_labels, _, _ = preprocess_messages(val_data) \n",
        "    test_msgs, test_labels, _, _ = preprocess_messages(test_data)\n",
        "    \n",
        "    # ...existing code...\n",
        "\n",
        "    train_msgs, train_labels, train_num_truthful, train_num_deceptive = preprocess_messages(train_data)\n",
        "    val_msgs, val_labels, _, _ = preprocess_messages(val_data)\n",
        "    test_msgs, test_labels, _, _ = preprocess_messages(test_data)\n",
        "\n",
        "    # Define harbingers\n",
        "    harbingers = [\n",
        "    # 1. Uncertainty & Probability (40 terms)\n",
        "    \"maybe\", \"perhaps\", \"possibly\", \"might\", \"could\", \"would\", \"should\",\n",
        "    \"potentially\", \"presumably\", \"likely\", \"unlikely\", \"probably\", \"possibly\",\n",
        "    \"conceivably\", \"hopefully\", \"eventually\", \"ultimately\", \"definitely\",\n",
        "    \"certainly\", \"surely\", \"absolutely\", \"undoubtedly\", \"clearly\", \"obviously\",\n",
        "    \"apparently\", \"seemingly\", \"allegedly\", \"supposedly\", \"reportedly\",\n",
        "    \"essentially\", \"basically\", \"fundamentally\", \"significantly\", \"considerably\",\n",
        "    \"virtually\", \"practically\", \"nearly\", \"almost\", \"marginally\", \"somewhat\",\n",
        "\n",
        "    # 2. Cognitive & Mental State Verbs (35 terms)\n",
        "    \"think\", \"believe\", \"feel\", \"suppose\", \"guess\", \"wonder\", \"assume\",\n",
        "    \"suspect\", \"estimate\", \"imagine\", \"figure\", \"reckon\", \"expect\", \"predict\",\n",
        "    \"anticipate\", \"foresee\", \"presume\", \"infer\", \"deduce\", \"conclude\",\n",
        "    \"gather\", \"surmise\", \"speculate\", \"theorize\", \"hypothesize\", \"sense\",\n",
        "    \"perceive\", \"notice\", \"realize\", \"recognize\", \"understand\", \"know\",\n",
        "    \"remember\", \"recall\", \"forget\",\n",
        "\n",
        "    # 3. Hedges & Approximators (30 terms)\n",
        "    \"sort\", \"kind\", \"rather\", \"quite\", \"somewhat\", \"slightly\", \"moderately\",\n",
        "    \"relatively\", \"comparatively\", \"reasonably\", \"fairly\", \"pretty\", \"mostly\",\n",
        "    \"mainly\", \"primarily\", \"partially\", \"largely\", \"substantially\", \"typically\",\n",
        "    \"generally\", \"usually\", \"normally\", \"commonly\", \"regularly\", \"often\",\n",
        "    \"frequently\", \"sometimes\", \"occasionally\", \"rarely\", \"seldom\",\n",
        "\n",
        "    # 4. Intensifiers & Emphatics (25 terms)\n",
        "    \"very\", \"really\", \"extremely\", \"absolutely\", \"totally\", \"completely\",\n",
        "    \"utterly\", \"perfectly\", \"entirely\", \"thoroughly\", \"fully\", \"wholly\",\n",
        "    \"downright\", \"positively\", \"simply\", \"just\", \"merely\", \"only\",\n",
        "    \"literally\", \"actually\", \"honestly\", \"truly\", \"genuinely\", \"sincerely\",\n",
        "    \"frankly\",\n",
        "\n",
        "    # 5. Evasive & Ambiguous Terms (30 terms)\n",
        "    \"about\", \"around\", \"approximately\", \"roughly\", \"nearly\", \"close to\",\n",
        "    \"in the range of\", \"something like\", \"or so\", \"more or less\", \"give or take\",\n",
        "    \"in general\", \"on the whole\", \"all things considered\", \"by and large\",\n",
        "    \"for the most part\", \"to some extent\", \"in some ways\", \"in a sense\",\n",
        "    \"in theory\", \"technically\", \"strictly speaking\", \"officially\",\n",
        "    \"formally\", \"nominally\", \"effectively\", \"in effect\", \"in principle\",\n",
        "    \"ideally\", \"theoretically\",\n",
        "\n",
        "    # 6. Time-based Hedges (20 terms)\n",
        "    \"now\", \"currently\", \"presently\", \"at present\", \"at the moment\",\n",
        "    \"these days\", \"lately\", \"recently\", \"in recent times\", \"over time\",\n",
        "    \"with time\", \"in time\", \"sooner or later\", \"eventually\", \"ultimately\",\n",
        "    \"in the end\", \"at the end of the day\", \"when all is said and done\",\n",
        "    \"in the long run\", \"in the final analysis\",\n",
        "\n",
        "    # 7. Source Distancing (15 terms)\n",
        "    \"according to\", \"as per\", \"based on\", \"in light of\", \"in view of\",\n",
        "    \"given that\", \"seeing as\", \"considering\", \"taking into account\",\n",
        "    \"from what I understand\", \"from what I gather\", \"from my perspective\",\n",
        "    \"in my opinion\", \"to my knowledge\", \"as far as I know\",\n",
        "\n",
        "    # 8. Diplomatic-Specific Terms (25 terms)\n",
        "    \"diplomatically\", \"strategically\", \"tactically\", \"politically\",\n",
        "    \"negotiable\", \"flexible\", \"adaptable\", \"revisable\", \"amendable\",\n",
        "    \"subject to\", \"conditional upon\", \"dependent on\", \"contingent on\",\n",
        "    \"pending\", \"awaiting\", \"considering\", \"reviewing\", \"evaluating\",\n",
        "    \"assessing\", \"monitoring\", \"observing\", \"watching\", \"tracking\",\n",
        "    \"following\", \"pursuant to\"\n",
        "]\n",
        "\n",
        "    # Build vocab with harbingers\n",
        "    vocab = Vocab(min_freq=2)\n",
        "    vocab.build_vocab(train_msgs, harbingers)\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "    # Calculate class weights (truthful / deceptive)\n",
        "    pos_weight = train_num_truthful / train_num_deceptive if train_num_deceptive > 0 else 1.0\n",
        "    print(f\"Positive weight (truthful/deceptive): {pos_weight:.4f}\")\n",
        "\n",
        "    # Datasets and Loaders\n",
        "    context_size = 2\n",
        "    max_len = 300\n",
        "    train_set = DiplomacyDataset(train_msgs, train_labels, vocab, context_size=context_size, max_len=max_len)\n",
        "    val_set = DiplomacyDataset(val_msgs, val_labels, vocab, context_size=context_size, max_len=max_len)\n",
        "    test_set = DiplomacyDataset(test_msgs, test_labels, vocab, context_size=context_size, max_len=max_len)\n",
        "\n",
        "    # Change DataLoader configurations\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)  # Changed from 2 to 0\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=0)  # Changed from 2 to 0\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=0)  # Changed from 2 to 0\n",
        "\n",
        "    # Model setup - using embed_dim=127 to make d_model=128 (divisible by 4 heads)\n",
        "    model = TransformerClassifier(\n",
        "        vocab_size=len(vocab),\n",
        "        embed_dim=127,  # Changed from 128 to 127 to make d_model=128\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        hidden_dim=256,\n",
        "        dropout=0.3,\n",
        "        max_len=max_len\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, dtype=torch.float).to(device))\n",
        "\n",
        "    # For tracking best model\n",
        "    best_val_f1 = 0\n",
        "    best_model_path = \"/kaggle/working/best_diplomacy_model.pth\" if os.path.exists(\"/kaggle/working\") else \"best_diplomacy_model.pth\"\n",
        "\n",
        "    # Training loop (20 epochs)\n",
        "    for epoch in range(8):\n",
        "        print(f\"\\nStarting Epoch {epoch+1}...\")\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_logits, val_true = evaluate(model, val_loader, device)\n",
        "        val_probs = torch.sigmoid(torch.tensor(val_logits)).numpy()\n",
        "        val_preds = (val_probs > 0.5).astype(int)\n",
        "        val_f1 = f1_score(val_true, val_preds)\n",
        "        print(f\"Validation F1 (threshold=0.5): {val_f1:.4f}\")\n",
        "\n",
        "        # Find best threshold for validation set\n",
        "        best_threshold, best_f1 = find_best_threshold(val_logits, val_true)\n",
        "        print(f\"Best threshold: {best_threshold:.4f}, Best F1: {best_f1:.4f}\")\n",
        "\n",
        "        # Save best model based on F1 with best threshold\n",
        "        if best_f1 > best_val_f1:\n",
        "            best_val_f1 = best_f1\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "    # Final evaluation on validation and test sets\n",
        "    for name, loader, labels in [(\"Validation\", val_loader, val_labels), (\"Test\", test_loader, test_labels)]:\n",
        "        logits, true = evaluate(model, loader, device)\n",
        "        probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "        preds = (probs > best_threshold).astype(int)\n",
        "        print_metrics(true, preds, probs, name)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_training()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook8cbd29d4fe",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7109647,
          "sourceId": 11359632,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7135202,
          "sourceId": 11393113,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "cuda_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
