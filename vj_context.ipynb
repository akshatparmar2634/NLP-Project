{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fac0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05923fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyDataset(Dataset):\n",
    "    def __init__(self, hdf5_file):\n",
    "        # Load the processed HDF5 data\n",
    "        self.data = pd.read_hdf(hdf5_file, key='diplomacy_data')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get message embedding, context embeddings, and label\n",
    "        message_emb = torch.tensor(self.data['message_embedding'].iloc[idx], dtype=torch.float32)\n",
    "        context_emb = torch.tensor(self.data['context_embeddings'].iloc[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.data['label'].iloc[idx], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'message_emb': message_emb,    # Shape: (300,)\n",
    "            'context_emb': context_emb,    # Shape: (context_window, 300)\n",
    "            'label': label                 # Scalar: 0 or 1\n",
    "        }\n",
    "\n",
    "# Example DataLoader setup\n",
    "def create_dataloaders(train_file, val_file, test_file, batch_size=32):\n",
    "    train_dataset = DiplomacyDataset(train_file)\n",
    "    val_dataset = DiplomacyDataset(val_file)\n",
    "    test_dataset = DiplomacyDataset(test_file)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb02d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyLieDetector(nn.Module):\n",
    "    def __init__(self, embedding_dim=300, hidden_dim=128, context_window=2, dropout=0.3):\n",
    "        super(DiplomacyLieDetector, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.context_window = context_window\n",
    "        \n",
    "        # Encoder for context messages (LSTM)\n",
    "        self.context_encoder = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_dim * 2 + embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 + embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  # Binary classification (true/false)\n",
    "        \n",
    "    def forward(self, message_emb, context_emb):\n",
    "        # message_emb: (batch_size, embedding_dim)\n",
    "        # context_emb: (batch_size, context_window, embedding_dim)\n",
    "        \n",
    "        # Encode context messages\n",
    "        # Shape: (batch_size, context_window, 2 * hidden_dim)\n",
    "        context_output, (hidden, _) = self.context_encoder(context_emb)\n",
    "        \n",
    "        # Use last hidden state from both directions\n",
    "        # Shape: (batch_size, 2 * hidden_dim)\n",
    "        context_summary = hidden[-2:].transpose(0, 1).contiguous().view(-1, 2 * self.hidden_dim)\n",
    "        \n",
    "        # Combine message embedding with context summary for attention\n",
    "        # Shape: (batch_size, 2 * hidden_dim + embedding_dim)\n",
    "        combined = torch.cat((message_emb, context_summary), dim=1)\n",
    "        \n",
    "        # Attention weights\n",
    "        # Shape: (batch_size, hidden_dim)\n",
    "        attn_weights = F.tanh(self.attention(combined))\n",
    "        \n",
    "        # Final classification\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02cef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa06a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            message_emb = batch['message_emb'].to(device)\n",
    "            context_emb = batch['context_emb'].to(device)\n",
    "            true_labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(message_emb, context_emb)\n",
    "            loss = criterion(outputs, true_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(true_labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            message_emb = batch['message_emb'].to(device)\n",
    "            context_emb = batch['context_emb'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(message_emb, context_emb)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc,_,_,_ = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65539daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/processed/train_processed.h5\"\n",
    "val_file = \"data/processed/val_processed.h5\"\n",
    "test_file = \"data/processed/test_processed.h5\"\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(train_file, val_file, test_file)\n",
    "\n",
    "# Initialize model\n",
    "model = DiplomacyLieDetector(embedding_dim=300, hidden_dim=128, context_window=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd6ed6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 65.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 0.2085, Train Acc: 0.9494\n",
      "Val Loss: 0.1982, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "Train Loss: 0.1958, Train Acc: 0.9510\n",
      "Val Loss: 0.1930, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 64.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "Train Loss: 0.1901, Train Acc: 0.9510\n",
      "Val Loss: 0.1998, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 65.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Train Loss: 0.1848, Train Acc: 0.9510\n",
      "Val Loss: 0.1997, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "Train Loss: 0.1818, Train Acc: 0.9515\n",
      "Val Loss: 0.1953, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 65.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "Train Loss: 0.1775, Train Acc: 0.9513\n",
      "Val Loss: 0.1997, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "Train Loss: 0.1704, Train Acc: 0.9519\n",
      "Val Loss: 0.2058, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 67.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "Train Loss: 0.1640, Train Acc: 0.9532\n",
      "Val Loss: 0.2075, Val Acc: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "Train Loss: 0.1596, Train Acc: 0.9544\n",
      "Val Loss: 0.2484, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 67.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "Train Loss: 0.1515, Train Acc: 0.9551\n",
      "Val Loss: 0.2514, Val Acc: 0.9503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 65.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "Train Loss: 0.1435, Train Acc: 0.9556\n",
      "Val Loss: 0.2514, Val Acc: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 67.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "Train Loss: 0.1357, Train Acc: 0.9577\n",
      "Val Loss: 0.2710, Val Acc: 0.9527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "Train Loss: 0.1273, Train Acc: 0.9603\n",
      "Val Loss: 0.2629, Val Acc: 0.9503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "Train Loss: 0.1217, Train Acc: 0.9619\n",
      "Val Loss: 0.2975, Val Acc: 0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "Train Loss: 0.1134, Train Acc: 0.9623\n",
      "Val Loss: 0.3227, Val Acc: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 63.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "Train Loss: 0.1090, Train Acc: 0.9640\n",
      "Val Loss: 0.3322, Val Acc: 0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 67.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "Train Loss: 0.0995, Train Acc: 0.9668\n",
      "Val Loss: 0.3069, Val Acc: 0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 66.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "Train Loss: 0.0912, Train Acc: 0.9690\n",
      "Val Loss: 0.3833, Val Acc: 0.9395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 65.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "Train Loss: 0.0866, Train Acc: 0.9702\n",
      "Val Loss: 0.3821, Val Acc: 0.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Training: 100%|██████████| 378/378 [00:05<00:00, 64.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "Train Loss: 0.0778, Train Acc: 0.9731\n",
      "Val Loss: 0.4547, Val Acc: 0.9364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trained_model = train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Load best model\n",
    "trained_model.load_state_dict(torch.load('best_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85bf09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Test Loss: 0.3161\n",
      "Test Accuracy: 0.9043\n",
      "Test Precision: 0.9043\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 0.9497\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_acc, test_precision, test_recall, test_f1 = evaluate_model(trained_model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7aecef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_classification_report(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset and print a detailed classification report.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        test_loader: DataLoader for test dataset\n",
    "        criterion: Loss function (e.g., nn.CrossEntropyLoss)\n",
    "        device: Device to run evaluation on (e.g., 'cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        None (prints the classification report)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    # Add tqdm progress bar for test evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n",
    "            message_emb = batch['message_emb'].to(device)\n",
    "            context_emb = batch['context_emb'].to(device)\n",
    "            true_labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(message_emb, context_emb)\n",
    "            loss = criterion(outputs, true_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(true_labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    \n",
    "    # Generate and print classification report\n",
    "    report = classification_report(labels, preds, target_names=['False', 'True'])\n",
    "    \n",
    "    print(\"\\nTest Set Evaluation Results:\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Binary Precision: {precision:.4f}\")\n",
    "    print(f\"Binary Recall: {recall:.4f}\")\n",
    "    print(f\"Binary F1-Score: {f1:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c322c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set: 100%|██████████| 79/79 [00:00<00:00, 116.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Results:\n",
      "Average Loss: 0.3161\n",
      "Accuracy: 0.9043\n",
      "Binary Precision: 0.9043\n",
      "Binary Recall: 1.0000\n",
      "Binary F1-Score: 0.9497\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       240\n",
      "        True       0.90      1.00      0.95      2268\n",
      "\n",
      "    accuracy                           0.90      2508\n",
      "   macro avg       0.45      0.50      0.47      2508\n",
      "weighted avg       0.82      0.90      0.86      2508\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print_test_classification_report(trained_model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdbd4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyDataset(Dataset):\n",
    "    def __init__(self, hdf5_file):\n",
    "        self.data = pd.read_hdf(hdf5_file, key='diplomacy_data')\n",
    "        self.message_embeddings = np.array(self.data['message_embedding'].tolist())\n",
    "        self.context_embeddings = np.array(self.data['context_embeddings'].tolist())\n",
    "        self.labels = self.data['label'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'message_embedding': torch.tensor(self.message_embeddings[idx], dtype=torch.float32),\n",
    "            'context_embeddings': torch.tensor(self.context_embeddings[idx], dtype=torch.float32),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b86fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, false_ratio=0.75):  # New param: false_ratio\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.false_ratio = false_ratio  # Fraction of batch that should be False\n",
    "        self.pos_indices = np.where(dataset.labels == 1)[0]\n",
    "        self.neg_indices = np.where(dataset.labels == 0)[0]\n",
    "        self.num_pos = len(self.pos_indices)\n",
    "        self.num_neg = len(self.neg_indices)\n",
    "        self.neg_per_batch = int(batch_size * false_ratio)\n",
    "        self.pos_per_batch = batch_size - self.neg_per_batch\n",
    "        self.batches_per_epoch = max(self.num_pos // self.pos_per_batch, self.num_neg // self.neg_per_batch)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.batches_per_epoch):\n",
    "            neg_batch = np.random.choice(self.neg_indices, min(self.neg_per_batch, self.num_neg), replace=True)\n",
    "            pos_batch = np.random.choice(self.pos_indices, min(self.pos_per_batch, self.num_pos), replace=True)\n",
    "            batch = np.concatenate([neg_batch, pos_batch])\n",
    "            np.random.shuffle(batch)\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfbf053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Scalar or tensor for class weights\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)  # alpha for True, 1-alpha for False\n",
    "        focal_loss = alpha_t * (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1be46251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyLieDetector(nn.Module):\n",
    "    def __init__(self, embedding_dim=300, hidden_dim=256, context_window=2, \n",
    "                 num_heads=4, num_encoder_layers=2, dropout=0.1):\n",
    "        super(DiplomacyLieDetector, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.context_window = context_window\n",
    "        \n",
    "        self.context_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim * 2 + embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim * 4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 + embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # Ensure single output for binary classification\n",
    "        \n",
    "    def forward(self, message_embedding, context_embeddings):\n",
    "        batch_size = message_embedding.size(0)\n",
    "        lstm_out, _ = self.context_lstm(context_embeddings)\n",
    "        context_vector = lstm_out[:, -1, :]  # (batch_size, hidden_dim * 2)\n",
    "        combined = torch.cat([message_embedding, context_vector], dim=1)  # (batch_size, embedding_dim + hidden_dim * 2)\n",
    "        combined = combined.unsqueeze(1)  # (batch_size, 1, embedding_dim + hidden_dim * 2)\n",
    "        \n",
    "        transformer_out = self.transformer_encoder(combined)  # (batch_size, 1, embedding_dim + hidden_dim * 2)\n",
    "        features = transformer_out.squeeze(1)  # (batch_size, embedding_dim + hidden_dim * 2)\n",
    "        \n",
    "        x = F.relu(self.fc1(features))  # (batch_size, hidden_dim)\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)  # (batch_size, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6b5fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader,alpha, num_epochs=10, learning_rate=0.001, use_focal_loss=True):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Compute class weights for weighted BCE\n",
    "    dataset = train_loader.dataset\n",
    "    num_pos = np.sum(dataset.labels)\n",
    "    num_neg = len(dataset.labels) - num_pos\n",
    "    pos_weight = torch.tensor([num_neg / num_pos], device=device) if num_pos > 0 else torch.tensor([1.0], device=device)\n",
    "    \n",
    "    # Choose loss function\n",
    "    if use_focal_loss:\n",
    "        criterion = FocalLoss(alpha=1-alpha, gamma=2.0)\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_samples = 0\n",
    "        \n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "        for batch in train_loader_tqdm:\n",
    "            message_emb = batch['message_embedding'].to(device)\n",
    "            context_emb = batch['context_embeddings'].to(device)\n",
    "            labels = batch['label'].to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(message_emb, context_emb)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            train_samples += labels.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            \n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "        \n",
    "        train_loss = train_loss / train_samples\n",
    "        train_acc = train_correct / train_samples\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        \n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_tqdm:\n",
    "                message_emb = batch['message_embedding'].to(device)\n",
    "                context_emb = batch['context_embeddings'].to(device)\n",
    "                labels = batch['label'].to(device).unsqueeze(1)\n",
    "                \n",
    "                outputs = model(message_emb, context_emb)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                val_samples += labels.size(0)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                \n",
    "                val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "        \n",
    "        val_loss = val_loss / val_samples\n",
    "        val_acc = val_correct / val_samples\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba0adf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, threshold=0.5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_probs = []  # Store probabilities instead of binary preds\n",
    "    all_labels = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            message_emb = batch['message_embedding'].to(device)\n",
    "            context_emb = batch['context_embeddings'].to(device)\n",
    "            labels = batch['label'].to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(message_emb, context_emb)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            batch_size = labels.size(0)\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            all_probs.extend(torch.argmax(probs, dim=1))\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels, dtype=int)\n",
    "    \n",
    "    # Try multiple thresholds to find best F1 for False\n",
    "    thresholds = [0.5, 0.3, 0.1]\n",
    "    best_f1_false = 0\n",
    "    best_threshold = 0.5\n",
    "    best_report = None\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        all_preds = (all_probs > thresh).astype(int)\n",
    "        report = classification_report(all_labels, all_preds, target_names=['False', 'True'], output_dict=True)\n",
    "        f1_false = report['False']['f1-score']\n",
    "        print(f\"\\nThreshold: {thresh}\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=['False', 'True']))\n",
    "        \n",
    "        if f1_false > best_f1_false:\n",
    "            best_f1_false = f1_false\n",
    "            best_threshold = thresh\n",
    "            best_report = classification_report(all_labels, all_preds, target_names=['False', 'True'])\n",
    "    \n",
    "    print(f\"\\nBest Threshold: {best_threshold}\")\n",
    "    print(\"Best Classification Report:\\n\", best_report)\n",
    "    accuracy = np.mean((all_probs > best_threshold).astype(int) == all_labels)\n",
    "    print(f\"Test Accuracy at Best Threshold: {accuracy:.4f}\")\n",
    "    \n",
    "    return all_probs, all_labels, best_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87108d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, test_loader, threshold=0.5):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     total_samples = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(test_loader):\n",
    "#             message_emb = batch['message_embedding'].to(device)\n",
    "#             context_emb = batch['context_embeddings'].to(device)\n",
    "#             labels = batch['label'].to(device).unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "            \n",
    "#             outputs = model(message_emb, context_emb)  # Shape: (batch_size, 1)\n",
    "#             probs = torch.sigmoid(outputs)  # Shape: (batch_size, 1)\n",
    "#             preds = (probs > threshold).float()  # Shape: (batch_size, 1)\n",
    "            \n",
    "#             batch_size = labels.size(0)\n",
    "#             total_samples += batch_size\n",
    "            \n",
    "#             # Debug per batch\n",
    "#             print(f\"Batch {i}:\")\n",
    "#             print(f\"  Labels shape: {labels.shape}, Preds shape: {preds.shape}\")\n",
    "#             print(f\"  Labels flattened: {labels.cpu().numpy().flatten().shape}\")\n",
    "#             preds = torch.argmax(probs, dim=1)  # Shape: (batch_size,)\n",
    "#             print(f\"  Preds shape after argmax: {preds.shape}\")\n",
    "            \n",
    "#             # Append batch data\n",
    "#             all_labels.extend(labels.cpu().numpy().flatten())\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "#     # Convert to numpy arrays\n",
    "#     all_preds = np.array(all_preds, dtype=int)\n",
    "#     all_labels = np.array(all_labels, dtype=int)\n",
    "    \n",
    "#     # Final debug output\n",
    "#     print(f\"Total samples processed: {total_samples}\")\n",
    "#     print(f\"Number of true labels: {len(all_labels)}\")\n",
    "#     print(f\"Number of predicted labels: {len(all_preds)}\")\n",
    "    \n",
    "#     if len(all_labels) != len(all_preds):\n",
    "#         raise ValueError(f\"Mismatch in sample counts: {len(all_labels)} labels vs {len(all_preds)} predictions\")\n",
    "    \n",
    "#     # Print classification report\n",
    "#     report = classification_report(all_labels, all_preds, target_names=['False', 'True'])\n",
    "#     print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "#     # Calculate and print accuracy\n",
    "#     accuracy = np.mean(all_preds == all_labels)\n",
    "#     print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     return all_preds, all_labels, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e92ff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha for Focal Loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 1435/1435 [01:59<00:00, 12.02it/s, loss=0.029] \n",
      "Epoch 1/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 92.76it/s, loss=0.035] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0290, Train Acc: 0.2503, Val Loss: 0.0393, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 1435/1435 [02:42<00:00,  8.84it/s, loss=0.0284]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 91.52it/s, loss=0.0375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.0288, Train Acc: 0.2500, Val Loss: 0.0436, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 1435/1435 [02:53<00:00,  8.25it/s, loss=0.0286]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 91.65it/s, loss=0.0362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.0287, Train Acc: 0.2500, Val Loss: 0.0415, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 1435/1435 [02:51<00:00,  8.37it/s, loss=0.0286]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 91.92it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0286, Train Acc: 0.2500, Val Loss: 0.0417, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 1435/1435 [02:56<00:00,  8.12it/s, loss=0.0287]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 90.49it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0286, Train Acc: 0.2500, Val Loss: 0.0416, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 1435/1435 [02:56<00:00,  8.11it/s, loss=0.0286]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 90.50it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0286, Train Acc: 0.2500, Val Loss: 0.0417, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 1435/1435 [04:31<00:00,  5.29it/s, loss=0.0286]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 88.83it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0286, Train Acc: 0.2500, Val Loss: 0.0417, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 1435/1435 [04:39<00:00,  5.14it/s, loss=0.0286]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 90.81it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0286, Train Acc: 0.2500, Val Loss: 0.0417, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 1435/1435 [05:34<00:00,  4.29it/s, loss=0.0286]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 41/41 [00:01<00:00, 39.87it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0286, Train Acc: 0.2500, Val Loss: 0.0417, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 1435/1435 [09:40<00:00,  2.47it/s, loss=0.0286]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 41/41 [00:00<00:00, 92.96it/s, loss=0.0363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0286, Train Acc: 0.2500, Val Loss: 0.0417, Val Acc: 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiplomacyLieDetector(\n",
       "  (context_lstm): LSTM(300, 256, batch_first=True, bidirectional=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=812, out_features=812, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=812, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=812, bias=True)\n",
       "        (norm1): LayerNorm((812,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((812,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=812, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = DiplomacyDataset('data/processed/train_processed.h5')\n",
    "val_dataset = DiplomacyDataset('data/processed/val_processed.h5')\n",
    "test_dataset = DiplomacyDataset('data/processed/test_processed.h5')\n",
    "    \n",
    "# Create balanced batch sampler\n",
    "batch_size = 32\n",
    "train_sampler = BalancedBatchSampler(train_dataset, batch_size, false_ratio=0.75)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_pos = np.sum(train_dataset.labels)\n",
    "num_neg = len(train_dataset.labels) - num_pos\n",
    "alpha = num_neg / (num_pos + num_neg)  # Proportion of False\n",
    "print(f\"Alpha for Focal Loss: {alpha:.4f}\")\n",
    "\n",
    "# Initialize model\n",
    "model = DiplomacyLieDetector(\n",
    "    embedding_dim=300,\n",
    "    hidden_dim=256,\n",
    "    context_window=2,\n",
    "    num_heads=4,\n",
    "    num_encoder_layers=2\n",
    ")\n",
    "\n",
    "# criterion = FocalLoss(alpha=1-alpha)  # Use Focal Loss with alpha\n",
    "\n",
    "# Train with Focal Loss\n",
    "train_model(model, train_loader,val_loader, alpha, num_epochs=10, use_focal_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48d7bf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       240\n",
      "        True       0.90      1.00      0.95      2268\n",
      "\n",
      "    accuracy                           0.90      2508\n",
      "   macro avg       0.45      0.50      0.47      2508\n",
      "weighted avg       0.82      0.90      0.86      2508\n",
      "\n",
      "\n",
      "Threshold: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       240\n",
      "        True       0.90      1.00      0.95      2268\n",
      "\n",
      "    accuracy                           0.90      2508\n",
      "   macro avg       0.45      0.50      0.47      2508\n",
      "weighted avg       0.82      0.90      0.86      2508\n",
      "\n",
      "\n",
      "Threshold: 0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       240\n",
      "        True       0.90      1.00      0.95      2268\n",
      "\n",
      "    accuracy                           0.90      2508\n",
      "   macro avg       0.45      0.50      0.47      2508\n",
      "weighted avg       0.82      0.90      0.86      2508\n",
      "\n",
      "\n",
      "Best Threshold: 0.5\n",
      "Best Classification Report:\n",
      " None\n",
      "Test Accuracy at Best Threshold: 0.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vimal\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "preds, labels, report = evaluate_model(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca7e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
